{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 8: Applications of Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in Section 8.6 in course notes\n",
    "Suppose we have a univariate function given by: $f(x)=4x^2-4x+3$.<br>\n",
    "Let us choose to start with $x_0=2$ and learning rate $\\alpha = 0.1$. Suppose also that we choose a threshold (also called **epsilon**) of 0.001, meaning that we stop the iterations when the difference between two x-values is less than epsilon.<br>\n",
    "Recall that $\\frac{df}{dx}=f'(x)=8x-4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 2 # Starting value of x\n",
    "rate = 0.1 # Set learning rate\n",
    "epsilon = 0.001 # Stop algorithm when absolute difference between 2 consecutive x-values is less than epsilon\n",
    "diff = 1 # difference between 2 consecutive iterates\n",
    "max_iter = 1000 # set maximum number of iterations\n",
    "iter = 1 # iterations counter\n",
    "f = lambda x: 4*x**2-4*x+3\n",
    "deriv = lambda x: 8*x-4 # derivative of f\n",
    "\n",
    "# Now Gradient Descent\n",
    "\n",
    "while diff > epsilon and iter < max_iter:\n",
    "    x_new = x - rate * deriv(x)\n",
    "    print(\"Iteration \", iter, \": x-value is: \", x_new,\"f(x) is: \", f(x_new) )\n",
    "    diff = abs(x_new - x)\n",
    "    iter = iter + 1\n",
    "    x = x_new\n",
    "    \n",
    "print(\"The local minimum occurs at: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11\n",
    "Repeat the Example with $f(x)=4x^2-4x+3$ above, starting with $x_0=2$, but this time with a learning rate of $\\alpha = 0.001$. Choose an epsilon value of 0.000001 and set the maximum number of iterations to be 2000. Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2 # Starting value of x\n",
    "rate = 0.001 # Set learning rate\n",
    "epsilon = 0.000001 # Stop algorithm when absolute difference between 2 consecutive x-values is less than epsilon\n",
    "diff = 1 # difference between 2 consecutive iterates\n",
    "max_iter = 2000 # set maximum number of iterations\n",
    "iter = 1 # iterations counter\n",
    "f = lambda x: 4*x**2-4*x+3\n",
    "deriv = lambda x: 8*x-4 # derivative of f\n",
    "\n",
    "# Now Gradient Descent\n",
    "\n",
    "while diff > epsilon and iter < max_iter:\n",
    "    x_new = x - rate * deriv(x)\n",
    "    print(\"Iteration \", iter, \": x-value is: \", x_new,\"f(x) is: \", f(x_new) )\n",
    "    diff = abs(x_new - x)\n",
    "    iter = iter + 1\n",
    "    x = x_new\n",
    "    \n",
    "print(\"The local minimum occurs at: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 12\n",
    "Let $f(x)=x^5-30x^3+50x$. Use gradient descent to find the minimum of $f$ with each of the following.<br>\n",
    "(a) Start with $x_0=0$<br>\n",
    "(b) Start with $x_0=2$<br>\n",
    "In each, use a learning rate of 0.001, epsilon 0.001, and set the maximum number of iterations to 1000. Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 0 # Starting value of x\n",
    "rate = 0.001 # Set learning rate\n",
    "precision = 0.001 # Stop algorithm when absolute difference between 2 consecutive x-values is less than precision\n",
    "diff = 1 # difference between 2 consecutive iterates\n",
    "max_iter = 1000 # set maximum number of iterations\n",
    "iter = 1 # iterations counter\n",
    "f = lambda x: x**5-30*x**3+50*x\n",
    "deriv = lambda x: 5*x**4-90*x**2+50 # derivative of f\n",
    "\n",
    "# Now Gradient Descent\n",
    "\n",
    "while diff > precision and iter < max_iter:\n",
    "    x_new = x - rate * deriv(x)\n",
    "    print(\"Iteration \", iter, \": x-value is: \", x_new,\"f(x) is: \", f(x_new) )\n",
    "    diff = abs(x_new - x)\n",
    "    iter = iter + 1\n",
    "    x = x_new\n",
    "    \n",
    "print(\"The local minimum occurs at: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part(b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2 # Starting value of x\n",
    "rate = 0.001 # Set learning rate\n",
    "precision = 0.001 # Stop algorithm when absolute difference between 2 consecutive x-values is less than precision\n",
    "diff = 1 # difference between 2 consecutive iterates\n",
    "max_iter = 1000 # set maximum number of iterations\n",
    "iter = 1 # iterations counter\n",
    "f = lambda x: x**5-30*x**3+50*x\n",
    "deriv = lambda x: 5*x**4-90*x**2+50 # derivative of f\n",
    "\n",
    "# Now Gradient Descent\n",
    "\n",
    "while diff > precision and iter < max_iter:\n",
    "    x_new = x - rate * deriv(x)\n",
    "    print(\"Iteration \", iter, \": x-value is: \", x_new,\"f(x) is: \", f(x_new) )\n",
    "    diff = abs(x_new - x)\n",
    "    iter = iter + 1\n",
    "    x = x_new\n",
    "    \n",
    "print(\"The local minimum occurs at: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph for Example 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.linspace(-6,6,100)\n",
    "y = x**5 -30*x**3+50*x\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Matplotlib in Python to graph each of the following functions $f$. Then estimate the global\n",
    "maximum and minimum values of $f$ (if any) on the given interval. Also state where those values \n",
    "occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) $f(x)=4x^3-3x^4$; $(-\\infty, \\infty)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $f(x)=(x^2-1)^2$; $(-\\infty, \\infty)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) $f(x)=1+\\frac{1}{x}$; $(0, \\infty)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(x)=x^4-3x^3+15$. Implement gradient descent in Python to find the global minimum of $f$, and\n",
    "state where it occurs.<br><br>\n",
    "(a) Start the search at $x_0=6$ with the following settings: learning rate $\\alpha=0.01$, epsilon =  0.00001, and the maximum number of iterations = 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Start the search at $x_0=6$ with the following settings: learning rate $\\alpha=0.001$, epsilon =  0.00001, and the maximum number of iterations = 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Start the search at $x_0=10$ with the following settings: learning rate $\\alpha=0.01$, epsilon =  0.00001, and the maximum number of iterations = 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Start the search at $x_0=-2$ with the following settings: learning rate $\\alpha=0.01$, epsilon =  0.00001, and the maximum number of iterations = 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
