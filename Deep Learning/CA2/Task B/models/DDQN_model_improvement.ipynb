{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# pip install matplotlib"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZEZsl5Yh96iW"},"outputs":[],"source":["import os\n","import json\n","import numpy as np\n","import gymnasium as gym\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","import PIL.ImageDraw as ImageDraw\n","import imageio\n","import os\n","import wandb\n","import numpy as np\n","from time import time\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.optimizers import Adam\n","# from DQN_analysis import DQN\n","env = gym.make('Pendulum-v1', g=9.81,render_mode=\"rgb_array\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"z59Sp7JM--PU"},"outputs":[],"source":["lr = 0.0009\n","epsilon = 1.0\n"," # we want epsilon to be 0.01 after n episodes\n","gamma = 0.99\n","training_episodes = 200\n","epsilon_decay = (0.01 / epsilon) ** (1/150) \n","target_update_interval = 5"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, max_length, state_size, action_size, is_sarsa=False):\n","        self.is_sarsa = is_sarsa\n","        self.memory_counter = 0\n","        self.max_length = max_length\n","        self.state_memory = np.zeros((self.max_length, state_size))\n","        self.new_state_memory = np.zeros((self.max_length, state_size))\n","        self.action_memory = np.zeros((self.max_length, action_size), dtype=np.int8)\n","        if is_sarsa:\n","            self.new_action_memory = np.zeros((self.max_length, action_size), dtype=np.int8)\n","        self.reward_memory = np.zeros(self.max_length)\n","        self.done_memory = np.zeros(self.max_length, dtype=np.float32)\n","\n","    def append(self, state, action, reward, new_state, done, new_action=None):\n","        idx = self.memory_counter % self.max_length\n","\n","        self.state_memory[idx] = state\n","        self.action_memory[idx] = action  # Assuming action is a single float\n","\n","        if self.is_sarsa:\n","            self.new_action_memory[idx] = new_action  # Assuming new_action is a single float\n","\n","        self.new_state_memory[idx] = new_state\n","        self.reward_memory[idx] = reward\n","        self.done_memory[idx] = 1 - done\n","        self.memory_counter += 1\n","\n","    def sample(self, batch_size):\n","        max_memory = min(self.memory_counter, self.max_length)\n","        sampled_batch = np.random.choice(max_memory, batch_size, replace=False)\n","\n","        states= self.state_memory[sampled_batch]\n","        actions = self.action_memory[sampled_batch]\n","        rewards= self.reward_memory[sampled_batch]\n","        new_states = self.new_state_memory[sampled_batch]\n","        if self.is_sarsa:\n","            new_actions = self.new_action_memory[sampled_batch]\n","        dones = self.done_memory[sampled_batch]\n","\n","        if not self.is_sarsa:\n","            return states, actions, rewards, new_states, dones\n","        else:\n","            return states, actions, rewards, new_states, new_actions, dones"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class MyEpisodeSaver:\n","    def __init__(self, env, frames, algo, episode_number):\n","        self.env = env\n","        self.frames = frames\n","        self.dir = f'./gifs/{algo}/'\n","        self.episode_number = episode_number\n","        self.fname = f'episode_{self.episode_number}.gif'\n","\n","        if not os.path.exists('./gifs'):\n","            os.mkdir('./gifs')\n","\n","        if not os.path.exists(self.dir):\n","            os.mkdir(self.dir)\n","\n","        self.labeled_frames = self.label_frames()\n","\n","    def label_frames(self):\n","        labeled_frames = []\n","\n","        for frame in self.frames:\n","            img = Image.fromarray(frame)\n","            draw = ImageDraw.Draw(img)\n","            # draw on each frame\n","            draw.text((10, 10), f'Episode: {self.episode_number}', fill=(255, 255, 255))\n","            labeled_frames.append(np.array(img))\n","\n","        return labeled_frames\n","\n","    def save(self):\n","        # labeled_frames = self.label_frames()\n","        # imageio.mimsave(self.dir + self.fname, labeled_frames, fps=60)\n","        imageio.mimsave(self.dir + self.fname, self.labeled_frames, fps=60)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import imageio, random\n","import pandas as pd\n","from keras.regularizers import l2\n","\n","class DDQN:\n","    def __init__(self, env, lr, gamma, epsilon, epsilon_decay, epsilon_min=0.01, batch_size=128, fname='DDQN_model_improvement'):\n","        self.env = env\n","        self.action_size = 10\n","        self.state_size = env.observation_space.shape[0]\n","        self.action_space = [i for i in range(self.action_size)] \n","        self.discrete_actions = np.linspace(-2.0, 2.0, num = self.action_size)\n","\n","        self.alpha = lr  # learning rate\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.tau = 0.01\n","        self.epsilon_decay = epsilon_decay\n","        self.epsilon_min = epsilon_min\n","        self.batch_size = batch_size\n","        self.target_update_interval = 100\n","\n","        self.fname = fname\n","        self.memory = ReplayBuffer(10000, self.state_size, self.action_size)\n","        self.dqn_model = self.create_dqn('DQN-Model')\n","        self.update_counter = 0\n","        self.dqn_target_model = self.create_dqn('DQN-Target-Model')\n","\n","\n","    def create_dqn(self, name):\n","        model = Sequential([\n","            Dense(units=64, activation='leaky_relu', input_shape=(self.state_size,), kernel_regularizer=l2(0.01)),\n","            Dense(units=32, activation='leaky_relu', kernel_regularizer=l2(0.01)),\n","            Dense(units=self.action_size, activation='linear', kernel_regularizer=l2(0.01))\n","        ], name=name)\n","\n","        model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha))\n","        return model\n","\n","    def remember(self, state, action, reward, new_state, done):\n","        self.memory.append(state, action, reward, new_state, done)\n","\n","    def act(self, state):\n","        state = np.reshape(state, [1, self.state_size])\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        else:\n","            q_values = self.dqn_model.predict(state, verbose=0)\n","            return np.argmax(q_values[0])\n","\n","        # q_values = self.dqn_model.predict(state, verbose=0)[0]\n","\n","        # # Sample action from a normal distribution centered around the action with the highest Q-value\n","        # chosen_action = np.random.normal(loc=self.action_space[np.argmax(q_values)], scale=self.epsilon)\n","\n","        # # Clip the action to be within the action space\n","        # chosen_action = np.clip(chosen_action, self.action_space.min(), self.action_space.max())\n","\n","        # return chosen_action\n","\n","    def update(self):\n","        if self.memory.memory_counter > self.batch_size:\n","            state, action, reward, new_state, done = self.memory.sample(self.batch_size)\n","\n","            q_current = self.dqn_target_model.predict(new_state, verbose=0) \n","            q_future = self.dqn_model.predict(new_state, verbose=0)\n","            q_target = self.dqn_model.predict(state, verbose=0)\n","\n","            best_actions = np.argmax(q_current, axis=1)\n","\n","            for i in range(self.batch_size):\n","                q_target[i, action[i]] = reward[i] + self.gamma * q_future[i, best_actions[i]] * done[i]\n","\n","            self.dqn_model.fit(x=state, y=q_target, verbose=0)\n","            self.update_counter += 1\n","\n","            # soft update with tau\n","            main_weights = self.dqn_model.get_weights()\n","            target_weights = self.dqn_target_model.get_weights()\n","\n","            for i in range(len(target_weights)):\n","                target_weights[i] = self.tau * main_weights[i] + (1 - self.tau) * target_weights[i]\n","            \n","            self.dqn_target_model.set_weights(target_weights)\n","\n","            # # Soft target update with tau\n","            # if self.update_counter % self.target_update_interval == 0:\n","            #     main_weights = self.dqn_model.get_weights()\n","            #     target_weights = self.dqn_target_model.get_weights()\n","\n","            #     new_weights = [(1 - self.tau) * target_w + self.tau * main_w for target_w, main_w in zip(target_weights, main_weights)]\n","            #     self.dqn_target_model.set_weights(new_weights)\n","\n","    def train(self, n_episodes, max_steps=200, log_wandb=False,\n","              update=True, save_episodes=True, save_interval=10):\n","        history = {'reward': [], 'avg_reward_100': [], 'steps': []}\n","\n","        for episode in range(n_episodes):\n","            # print(episode)\n","            start_time = time()\n","            state, info = self.env.reset()\n","            state = np.reshape(state, [1, self.state_size])\n","            done = False\n","            episode_reward = 0\n","            episode_steps = 0\n","            episode_frames  = []\n","            # self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n","\n","            for _ in range(max_steps):\n","                action = self.act(state)\n","\n","                new_state, reward, done, _, _ = self.env.step([self.discrete_actions[action]]) # 5 values returned\n","                \n","                theta = new_state[0]\n","                theta_dt = new_state[1]\n","\n","                # Modify our reward\n","                reward = reward + (0.05 * (np.pi - np.abs(theta)))\n","                # reward = reward - (0.01 * np.abs(theta_dt))\n","\n","                new_state = np.reshape(new_state, [1, self.state_size])\n","                episode_frames.append(self.env.render())\n","\n","                if update:\n","                    self.remember(state, action, reward, new_state, done)\n","                    self.update()\n","\n","                state = new_state\n","                episode_reward += reward\n","                episode_steps += 1\n","\n","                if done:\n","                    break\n","\n","            if log_wandb:\n","                wandb.log({\n","                    'reward': episode_reward,\n","                    'steps': episode_steps,\n","                    'epsilon': self.epsilon\n","                })\n","\n","            if save_episodes:\n","                if (episode + 1) % save_interval == 0 or (episode == 0):\n","                    s = MyEpisodeSaver(self.env, episode_frames , self.fname, episode + 1)\n","                    s.save()\n","\n","            print(f'[EP {episode + 1}/{n_episodes}] - Reward: {episode_reward:.4f} - Steps: {episode_steps} - Eps: {self.epsilon:.4f} - Time: {time() - start_time:.2f}s')\n","            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n","            history['reward'].append(episode_reward)\n","            history['avg_reward_100'].append(np.mean(history['reward'][-100:]))\n","            history['steps'].append(episode_steps)\n","            df = pd.DataFrame(history)\n","            df.to_csv(f'./assets/{self.fname}.csv')\n","\n","        self.env.close()\n","\n","        if log_wandb:\n","            wandb.finish()\n","\n","        self.save(f'{self.fname}.h5')\n","\n","        return history\n","\n","    def save(self, fname):\n","        if not os.path.exists('./assets'):\n","            os.mkdir('./assets')\n","\n","        self.dqn_model.save(f'./assets/{fname}')\n","\n","    def load(self, fname):\n","        self.dqn_model = load_model(f'./assets/{fname}')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# pip install gymnasium[classic-control]"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JgCZtCq3_ChU"},"outputs":[{"name":"stdout","output_type":"stream","text":["[EP 1/200] - Reward: -1364.7476 - Steps: 200 - Eps: 1.0000 - Time: 16.06s\n","[EP 2/200] - Reward: -1524.8235 - Steps: 200 - Eps: 0.9698 - Time: 33.10s\n","[EP 3/200] - Reward: -1668.3026 - Steps: 200 - Eps: 0.9404 - Time: 33.15s\n","[EP 4/200] - Reward: -994.0826 - Steps: 200 - Eps: 0.9120 - Time: 33.22s\n","[EP 5/200] - Reward: -955.3590 - Steps: 200 - Eps: 0.8844 - Time: 33.69s\n","[EP 6/200] - Reward: -1258.7390 - Steps: 200 - Eps: 0.8577 - Time: 33.85s\n","[EP 7/200] - Reward: -901.9402 - Steps: 200 - Eps: 0.8318 - Time: 34.18s\n","[EP 8/200] - Reward: -1247.6549 - Steps: 200 - Eps: 0.8066 - Time: 34.40s\n","[EP 9/200] - Reward: -1289.0888 - Steps: 200 - Eps: 0.7822 - Time: 34.85s\n","[EP 10/200] - Reward: -956.6223 - Steps: 200 - Eps: 0.7586 - Time: 35.19s\n","[EP 11/200] - Reward: -1162.7771 - Steps: 200 - Eps: 0.7356 - Time: 35.04s\n","[EP 12/200] - Reward: -988.5713 - Steps: 200 - Eps: 0.7134 - Time: 35.41s\n","[EP 13/200] - Reward: -991.2593 - Steps: 200 - Eps: 0.6918 - Time: 35.66s\n","[EP 14/200] - Reward: -1391.1684 - Steps: 200 - Eps: 0.6709 - Time: 35.50s\n","[EP 15/200] - Reward: -1151.6667 - Steps: 200 - Eps: 0.6506 - Time: 35.34s\n","[EP 16/200] - Reward: -1251.4198 - Steps: 200 - Eps: 0.6310 - Time: 36.15s\n","[EP 17/200] - Reward: -1295.2701 - Steps: 200 - Eps: 0.6119 - Time: 35.78s\n","[EP 18/200] - Reward: -1292.2988 - Steps: 200 - Eps: 0.5934 - Time: 36.16s\n","[EP 19/200] - Reward: -1185.8989 - Steps: 200 - Eps: 0.5754 - Time: 35.81s\n","[EP 20/200] - Reward: -1188.8007 - Steps: 200 - Eps: 0.5580 - Time: 39.39s\n","[EP 21/200] - Reward: -754.0424 - Steps: 200 - Eps: 0.5412 - Time: 36.68s\n","[EP 22/200] - Reward: -1078.2301 - Steps: 200 - Eps: 0.5248 - Time: 36.67s\n","[EP 23/200] - Reward: -881.3765 - Steps: 200 - Eps: 0.5089 - Time: 37.10s\n","[EP 24/200] - Reward: -867.1906 - Steps: 200 - Eps: 0.4936 - Time: 36.90s\n","[EP 25/200] - Reward: -854.9916 - Steps: 200 - Eps: 0.4786 - Time: 37.06s\n","[EP 26/200] - Reward: -955.8388 - Steps: 200 - Eps: 0.4642 - Time: 37.75s\n","[EP 27/200] - Reward: -833.4274 - Steps: 200 - Eps: 0.4501 - Time: 37.50s\n","[EP 28/200] - Reward: -841.6025 - Steps: 200 - Eps: 0.4365 - Time: 37.54s\n","[EP 29/200] - Reward: -744.2011 - Steps: 200 - Eps: 0.4233 - Time: 36.64s\n","[EP 30/200] - Reward: -853.7047 - Steps: 200 - Eps: 0.4105 - Time: 37.15s\n","[EP 31/200] - Reward: -623.4803 - Steps: 200 - Eps: 0.3981 - Time: 37.55s\n","[EP 32/200] - Reward: -721.2865 - Steps: 200 - Eps: 0.3861 - Time: 37.43s\n","[EP 33/200] - Reward: -730.8110 - Steps: 200 - Eps: 0.3744 - Time: 37.62s\n","[EP 34/200] - Reward: -731.3507 - Steps: 200 - Eps: 0.3631 - Time: 38.04s\n","[EP 35/200] - Reward: -730.9696 - Steps: 200 - Eps: 0.3521 - Time: 38.08s\n","[EP 36/200] - Reward: -723.8631 - Steps: 200 - Eps: 0.3415 - Time: 37.74s\n","[EP 37/200] - Reward: -727.8853 - Steps: 200 - Eps: 0.3311 - Time: 38.39s\n","[EP 38/200] - Reward: -611.0633 - Steps: 200 - Eps: 0.3211 - Time: 38.28s\n","[EP 39/200] - Reward: -663.8710 - Steps: 200 - Eps: 0.3114 - Time: 38.62s\n","[EP 40/200] - Reward: -611.8787 - Steps: 200 - Eps: 0.3020 - Time: 41.84s\n","[EP 41/200] - Reward: -607.8311 - Steps: 200 - Eps: 0.2929 - Time: 38.96s\n","[EP 42/200] - Reward: -612.0386 - Steps: 200 - Eps: 0.2840 - Time: 39.80s\n","[EP 43/200] - Reward: -678.0072 - Steps: 200 - Eps: 0.2754 - Time: 38.61s\n","[EP 44/200] - Reward: -604.5427 - Steps: 200 - Eps: 0.2671 - Time: 38.82s\n","[EP 45/200] - Reward: -716.1540 - Steps: 200 - Eps: 0.2590 - Time: 39.19s\n","[EP 46/200] - Reward: -601.4728 - Steps: 200 - Eps: 0.2512 - Time: 38.92s\n","[EP 47/200] - Reward: -724.8992 - Steps: 200 - Eps: 0.2436 - Time: 38.97s\n","[EP 48/200] - Reward: -571.8150 - Steps: 200 - Eps: 0.2362 - Time: 39.43s\n","[EP 49/200] - Reward: -612.2726 - Steps: 200 - Eps: 0.2291 - Time: 39.34s\n","[EP 50/200] - Reward: -616.9690 - Steps: 200 - Eps: 0.2222 - Time: 39.73s\n","[EP 51/200] - Reward: -497.0142 - Steps: 200 - Eps: 0.2154 - Time: 39.36s\n","[EP 52/200] - Reward: -490.0787 - Steps: 200 - Eps: 0.2089 - Time: 39.78s\n","[EP 53/200] - Reward: -491.9564 - Steps: 200 - Eps: 0.2026 - Time: 39.63s\n","[EP 54/200] - Reward: -360.0573 - Steps: 200 - Eps: 0.1965 - Time: 40.03s\n","[EP 55/200] - Reward: -471.7756 - Steps: 200 - Eps: 0.1905 - Time: 39.86s\n","[EP 56/200] - Reward: -469.3089 - Steps: 200 - Eps: 0.1848 - Time: 40.41s\n","[EP 57/200] - Reward: -475.2352 - Steps: 200 - Eps: 0.1792 - Time: 40.06s\n","[EP 58/200] - Reward: -438.3910 - Steps: 200 - Eps: 0.1738 - Time: 39.64s\n","[EP 59/200] - Reward: -358.9622 - Steps: 200 - Eps: 0.1685 - Time: 40.32s\n","[EP 60/200] - Reward: -239.9216 - Steps: 200 - Eps: 0.1634 - Time: 44.62s\n","[EP 61/200] - Reward: -231.0691 - Steps: 200 - Eps: 0.1585 - Time: 40.45s\n","[EP 62/200] - Reward: -105.8655 - Steps: 200 - Eps: 0.1537 - Time: 40.70s\n","[EP 63/200] - Reward: -108.8697 - Steps: 200 - Eps: 0.1491 - Time: 40.39s\n","[EP 64/200] - Reward: -607.4504 - Steps: 200 - Eps: 0.1445 - Time: 39.97s\n","[EP 65/200] - Reward: -457.3548 - Steps: 200 - Eps: 0.1402 - Time: 40.26s\n","[EP 66/200] - Reward: -103.0848 - Steps: 200 - Eps: 0.1359 - Time: 40.32s\n","[EP 67/200] - Reward: -101.1069 - Steps: 200 - Eps: 0.1318 - Time: 40.81s\n","[EP 68/200] - Reward: -342.5864 - Steps: 200 - Eps: 0.1278 - Time: 40.44s\n","[EP 69/200] - Reward: -111.4926 - Steps: 200 - Eps: 0.1240 - Time: 40.55s\n","[EP 70/200] - Reward: -363.1462 - Steps: 200 - Eps: 0.1202 - Time: 40.54s\n","[EP 71/200] - Reward: -242.2512 - Steps: 200 - Eps: 0.1166 - Time: 40.60s\n","[EP 72/200] - Reward: -676.7481 - Steps: 200 - Eps: 0.1131 - Time: 40.94s\n","[EP 73/200] - Reward: -1361.3831 - Steps: 200 - Eps: 0.1096 - Time: 41.28s\n","[EP 74/200] - Reward: -1053.8332 - Steps: 200 - Eps: 0.1063 - Time: 41.06s\n","[EP 75/200] - Reward: 17.1856 - Steps: 200 - Eps: 0.1031 - Time: 41.31s\n","[EP 76/200] - Reward: 18.7387 - Steps: 200 - Eps: 0.1000 - Time: 42.40s\n","[EP 77/200] - Reward: -100.2685 - Steps: 200 - Eps: 0.0970 - Time: 42.22s\n","[EP 78/200] - Reward: -1171.9814 - Steps: 200 - Eps: 0.0940 - Time: 41.97s\n","[EP 79/200] - Reward: 19.7666 - Steps: 200 - Eps: 0.0912 - Time: 41.13s\n","[EP 80/200] - Reward: -98.0026 - Steps: 200 - Eps: 0.0884 - Time: 45.41s\n","[EP 81/200] - Reward: -752.4592 - Steps: 200 - Eps: 0.0858 - Time: 41.35s\n","[EP 82/200] - Reward: 19.6292 - Steps: 200 - Eps: 0.0832 - Time: 41.04s\n","[EP 83/200] - Reward: -104.0514 - Steps: 200 - Eps: 0.0807 - Time: 41.16s\n","[EP 84/200] - Reward: -525.8150 - Steps: 200 - Eps: 0.0782 - Time: 41.42s\n","[EP 85/200] - Reward: -103.8566 - Steps: 200 - Eps: 0.0759 - Time: 41.15s\n","[EP 86/200] - Reward: -103.6893 - Steps: 200 - Eps: 0.0736 - Time: 41.60s\n","[EP 87/200] - Reward: -243.4853 - Steps: 200 - Eps: 0.0713 - Time: 41.61s\n","[EP 88/200] - Reward: -502.6386 - Steps: 200 - Eps: 0.0692 - Time: 41.54s\n","[EP 89/200] - Reward: -230.3834 - Steps: 200 - Eps: 0.0671 - Time: 41.99s\n","[EP 90/200] - Reward: -342.1476 - Steps: 200 - Eps: 0.0651 - Time: 41.86s\n","[EP 91/200] - Reward: -107.4498 - Steps: 200 - Eps: 0.0631 - Time: 41.84s\n","[EP 92/200] - Reward: -223.8671 - Steps: 200 - Eps: 0.0612 - Time: 41.67s\n","[EP 93/200] - Reward: -109.3249 - Steps: 200 - Eps: 0.0593 - Time: 41.88s\n","[EP 94/200] - Reward: -476.7740 - Steps: 200 - Eps: 0.0575 - Time: 42.09s\n","[EP 95/200] - Reward: -100.4301 - Steps: 200 - Eps: 0.0558 - Time: 42.91s\n","[EP 96/200] - Reward: 19.1738 - Steps: 200 - Eps: 0.0541 - Time: 42.18s\n","[EP 97/200] - Reward: -218.9723 - Steps: 200 - Eps: 0.0525 - Time: 42.48s\n","[EP 98/200] - Reward: -347.6393 - Steps: 200 - Eps: 0.0509 - Time: 42.40s\n","[EP 99/200] - Reward: -104.9815 - Steps: 200 - Eps: 0.0494 - Time: 42.11s\n","[EP 100/200] - Reward: 18.0702 - Steps: 200 - Eps: 0.0479 - Time: 46.31s\n","[EP 101/200] - Reward: -105.5535 - Steps: 200 - Eps: 0.0464 - Time: 42.14s\n","[EP 102/200] - Reward: -462.1563 - Steps: 200 - Eps: 0.0450 - Time: 42.34s\n","[EP 103/200] - Reward: -102.1181 - Steps: 200 - Eps: 0.0437 - Time: 42.10s\n","[EP 104/200] - Reward: 19.7459 - Steps: 200 - Eps: 0.0423 - Time: 42.13s\n","[EP 105/200] - Reward: -103.4908 - Steps: 200 - Eps: 0.0411 - Time: 42.42s\n","[EP 106/200] - Reward: -358.9703 - Steps: 200 - Eps: 0.0398 - Time: 42.62s\n","[EP 107/200] - Reward: -455.6330 - Steps: 200 - Eps: 0.0386 - Time: 42.82s\n","[EP 108/200] - Reward: -105.0788 - Steps: 200 - Eps: 0.0374 - Time: 42.80s\n","[EP 109/200] - Reward: -101.8135 - Steps: 200 - Eps: 0.0363 - Time: 43.31s\n","[EP 110/200] - Reward: -100.5335 - Steps: 200 - Eps: 0.0352 - Time: 43.15s\n","[EP 111/200] - Reward: -98.5217 - Steps: 200 - Eps: 0.0341 - Time: 43.21s\n","[EP 112/200] - Reward: -103.0916 - Steps: 200 - Eps: 0.0331 - Time: 43.32s\n","[EP 113/200] - Reward: -376.9763 - Steps: 200 - Eps: 0.0321 - Time: 42.96s\n","[EP 114/200] - Reward: -102.2923 - Steps: 200 - Eps: 0.0311 - Time: 42.90s\n","[EP 115/200] - Reward: -207.8969 - Steps: 200 - Eps: 0.0302 - Time: 43.04s\n","[EP 116/200] - Reward: 18.7975 - Steps: 200 - Eps: 0.0293 - Time: 42.56s\n","[EP 117/200] - Reward: -96.3178 - Steps: 200 - Eps: 0.0284 - Time: 43.03s\n","[EP 118/200] - Reward: -98.8841 - Steps: 200 - Eps: 0.0275 - Time: 42.46s\n","[EP 119/200] - Reward: -103.2202 - Steps: 200 - Eps: 0.0267 - Time: 42.65s\n","[EP 120/200] - Reward: -232.1765 - Steps: 200 - Eps: 0.0259 - Time: 48.50s\n","[EP 121/200] - Reward: -103.2032 - Steps: 200 - Eps: 0.0251 - Time: 43.55s\n","[EP 122/200] - Reward: 19.3094 - Steps: 200 - Eps: 0.0244 - Time: 43.45s\n","[EP 123/200] - Reward: 18.4483 - Steps: 200 - Eps: 0.0236 - Time: 43.78s\n","[EP 124/200] - Reward: -228.8958 - Steps: 200 - Eps: 0.0229 - Time: 43.96s\n","[EP 125/200] - Reward: -99.8125 - Steps: 200 - Eps: 0.0222 - Time: 43.62s\n","[EP 126/200] - Reward: -560.7168 - Steps: 200 - Eps: 0.0215 - Time: 43.64s\n","[EP 127/200] - Reward: -99.6848 - Steps: 200 - Eps: 0.0209 - Time: 43.79s\n","[EP 128/200] - Reward: -96.2280 - Steps: 200 - Eps: 0.0203 - Time: 44.01s\n","[EP 129/200] - Reward: -101.8030 - Steps: 200 - Eps: 0.0196 - Time: 44.10s\n","[EP 130/200] - Reward: -1371.2371 - Steps: 200 - Eps: 0.0191 - Time: 43.78s\n","[EP 131/200] - Reward: -1072.9983 - Steps: 200 - Eps: 0.0185 - Time: 43.70s\n","[EP 132/200] - Reward: -103.8152 - Steps: 200 - Eps: 0.0179 - Time: 44.28s\n","[EP 133/200] - Reward: -334.7061 - Steps: 200 - Eps: 0.0174 - Time: 43.77s\n","[EP 134/200] - Reward: -101.9780 - Steps: 200 - Eps: 0.0169 - Time: 43.75s\n","[EP 135/200] - Reward: -101.9367 - Steps: 200 - Eps: 0.0163 - Time: 43.84s\n","[EP 136/200] - Reward: -214.7076 - Steps: 200 - Eps: 0.0158 - Time: 43.56s\n","[EP 137/200] - Reward: -104.8826 - Steps: 200 - Eps: 0.0154 - Time: 43.86s\n","[EP 138/200] - Reward: -437.0529 - Steps: 200 - Eps: 0.0149 - Time: 43.82s\n","[EP 139/200] - Reward: -101.8408 - Steps: 200 - Eps: 0.0145 - Time: 43.80s\n","[EP 140/200] - Reward: -239.6705 - Steps: 200 - Eps: 0.0140 - Time: 50.01s\n","[EP 141/200] - Reward: -108.4076 - Steps: 200 - Eps: 0.0136 - Time: 44.60s\n","[EP 142/200] - Reward: -222.0031 - Steps: 200 - Eps: 0.0132 - Time: 44.82s\n","[EP 143/200] - Reward: -103.4536 - Steps: 200 - Eps: 0.0128 - Time: 44.83s\n","[EP 144/200] - Reward: -219.8619 - Steps: 200 - Eps: 0.0124 - Time: 45.22s\n","[EP 145/200] - Reward: -111.0628 - Steps: 200 - Eps: 0.0120 - Time: 45.76s\n","[EP 146/200] - Reward: -94.1595 - Steps: 200 - Eps: 0.0117 - Time: 45.76s\n","[EP 147/200] - Reward: -338.1964 - Steps: 200 - Eps: 0.0113 - Time: 45.58s\n","[EP 148/200] - Reward: 19.2675 - Steps: 200 - Eps: 0.0110 - Time: 45.43s\n","[EP 149/200] - Reward: -96.8309 - Steps: 200 - Eps: 0.0106 - Time: 45.70s\n","[EP 150/200] - Reward: -206.7104 - Steps: 200 - Eps: 0.0103 - Time: 45.38s\n","[EP 151/200] - Reward: -103.2331 - Steps: 200 - Eps: 0.0100 - Time: 45.98s\n","[EP 152/200] - Reward: 17.4478 - Steps: 200 - Eps: 0.0100 - Time: 45.30s\n","[EP 153/200] - Reward: -108.5418 - Steps: 200 - Eps: 0.0100 - Time: 45.98s\n","[EP 154/200] - Reward: -230.3936 - Steps: 200 - Eps: 0.0100 - Time: 45.79s\n","[EP 155/200] - Reward: 19.6343 - Steps: 200 - Eps: 0.0100 - Time: 46.43s\n","[EP 156/200] - Reward: -516.5727 - Steps: 200 - Eps: 0.0100 - Time: 46.14s\n","[EP 157/200] - Reward: -108.7998 - Steps: 200 - Eps: 0.0100 - Time: 46.13s\n","[EP 158/200] - Reward: -665.6517 - Steps: 200 - Eps: 0.0100 - Time: 46.53s\n","[EP 159/200] - Reward: -803.2205 - Steps: 200 - Eps: 0.0100 - Time: 46.48s\n","[EP 160/200] - Reward: -97.0592 - Steps: 200 - Eps: 0.0100 - Time: 53.37s\n","[EP 161/200] - Reward: -102.0698 - Steps: 200 - Eps: 0.0100 - Time: 46.01s\n","[EP 162/200] - Reward: 19.2550 - Steps: 200 - Eps: 0.0100 - Time: 45.56s\n","[EP 163/200] - Reward: -103.7775 - Steps: 200 - Eps: 0.0100 - Time: 45.42s\n","[EP 164/200] - Reward: -101.1344 - Steps: 200 - Eps: 0.0100 - Time: 45.10s\n","[EP 165/200] - Reward: -235.1540 - Steps: 200 - Eps: 0.0100 - Time: 45.04s\n","[EP 166/200] - Reward: -329.4289 - Steps: 200 - Eps: 0.0100 - Time: 44.73s\n","[EP 167/200] - Reward: -222.2973 - Steps: 200 - Eps: 0.0100 - Time: 44.89s\n","[EP 168/200] - Reward: -221.2194 - Steps: 200 - Eps: 0.0100 - Time: 44.71s\n","[EP 169/200] - Reward: -209.1119 - Steps: 200 - Eps: 0.0100 - Time: 44.90s\n","[EP 170/200] - Reward: -100.8973 - Steps: 200 - Eps: 0.0100 - Time: 44.98s\n","[EP 171/200] - Reward: -100.9688 - Steps: 200 - Eps: 0.0100 - Time: 45.15s\n","[EP 172/200] - Reward: -304.6762 - Steps: 200 - Eps: 0.0100 - Time: 45.73s\n","[EP 173/200] - Reward: -602.3720 - Steps: 200 - Eps: 0.0100 - Time: 46.00s\n","[EP 174/200] - Reward: -228.1724 - Steps: 200 - Eps: 0.0100 - Time: 46.40s\n","[EP 175/200] - Reward: -223.9481 - Steps: 200 - Eps: 0.0100 - Time: 46.59s\n","[EP 176/200] - Reward: -95.9663 - Steps: 200 - Eps: 0.0100 - Time: 46.69s\n","[EP 177/200] - Reward: -226.2633 - Steps: 200 - Eps: 0.0100 - Time: 46.90s\n","[EP 178/200] - Reward: -563.6098 - Steps: 200 - Eps: 0.0100 - Time: 46.47s\n","[EP 179/200] - Reward: -370.1072 - Steps: 200 - Eps: 0.0100 - Time: 46.79s\n","[EP 180/200] - Reward: 17.5199 - Steps: 200 - Eps: 0.0100 - Time: 54.41s\n","[EP 181/200] - Reward: -254.2637 - Steps: 200 - Eps: 0.0100 - Time: 47.33s\n","[EP 182/200] - Reward: -314.3007 - Steps: 200 - Eps: 0.0100 - Time: 48.64s\n","[EP 183/200] - Reward: -94.7071 - Steps: 200 - Eps: 0.0100 - Time: 46.46s\n","[EP 184/200] - Reward: -99.8178 - Steps: 200 - Eps: 0.0100 - Time: 46.94s\n","[EP 185/200] - Reward: -99.2680 - Steps: 200 - Eps: 0.0100 - Time: 46.96s\n","[EP 186/200] - Reward: -113.5535 - Steps: 200 - Eps: 0.0100 - Time: 46.68s\n","[EP 187/200] - Reward: -1238.0777 - Steps: 200 - Eps: 0.0100 - Time: 46.86s\n","[EP 188/200] - Reward: -1238.9509 - Steps: 200 - Eps: 0.0100 - Time: 47.24s\n","[EP 189/200] - Reward: -1332.0034 - Steps: 200 - Eps: 0.0100 - Time: 47.17s\n","[EP 190/200] - Reward: -1181.6578 - Steps: 200 - Eps: 0.0100 - Time: 47.33s\n","[EP 191/200] - Reward: -103.1151 - Steps: 200 - Eps: 0.0100 - Time: 47.52s\n","[EP 192/200] - Reward: -891.3054 - Steps: 200 - Eps: 0.0100 - Time: 48.49s\n","[EP 193/200] - Reward: -106.5291 - Steps: 200 - Eps: 0.0100 - Time: 48.44s\n","[EP 194/200] - Reward: -97.5906 - Steps: 200 - Eps: 0.0100 - Time: 48.54s\n","[EP 195/200] - Reward: -105.6108 - Steps: 200 - Eps: 0.0100 - Time: 48.94s\n","[EP 196/200] - Reward: -215.5431 - Steps: 200 - Eps: 0.0100 - Time: 48.13s\n","[EP 197/200] - Reward: -104.6874 - Steps: 200 - Eps: 0.0100 - Time: 48.10s\n","[EP 198/200] - Reward: -94.7161 - Steps: 200 - Eps: 0.0100 - Time: 47.96s\n","[EP 199/200] - Reward: 18.4208 - Steps: 200 - Eps: 0.0100 - Time: 48.01s\n","[EP 200/200] - Reward: 17.7563 - Steps: 200 - Eps: 0.0100 - Time: 55.93s\n"]},{"data":{"text/plain":["{'reward': [-1364.7476202295459,\n","  -1524.8234689937105,\n","  -1668.3026265506135,\n","  -994.0826174615685,\n","  -955.3590457826151,\n","  -1258.7389856666064,\n","  -901.940193053861,\n","  -1247.6549181221058,\n","  -1289.088783954735,\n","  -956.6222575547481,\n","  -1162.7771157281743,\n","  -988.5713403646057,\n","  -991.2592778102706,\n","  -1391.168356505743,\n","  -1151.6666964610224,\n","  -1251.41981791936,\n","  -1295.2701146124516,\n","  -1292.2987916487596,\n","  -1185.8989291557982,\n","  -1188.8007456615883,\n","  -754.0424333784166,\n","  -1078.2300773329805,\n","  -881.3765264780086,\n","  -867.190618229208,\n","  -854.9915530148867,\n","  -955.8387656330233,\n","  -833.4274053401699,\n","  -841.6024795621739,\n","  -744.20110522944,\n","  -853.7047343065034,\n","  -623.4803195270993,\n","  -721.2865424892296,\n","  -730.8109654951394,\n","  -731.3506627435964,\n","  -730.9696240714501,\n","  -723.8630562864266,\n","  -727.885292898631,\n","  -611.0632675368742,\n","  -663.8710086733788,\n","  -611.8786916015102,\n","  -607.8311468986478,\n","  -612.0386123539918,\n","  -678.0072006014753,\n","  -604.5427194557884,\n","  -716.1540471130752,\n","  -601.4727923499952,\n","  -724.899167985302,\n","  -571.8150239712722,\n","  -612.2726018970224,\n","  -616.9690400841408,\n","  -497.0142239801791,\n","  -490.0786934913928,\n","  -491.9563632380785,\n","  -360.05728137678494,\n","  -471.7756473845424,\n","  -469.3089199956234,\n","  -475.2352292520873,\n","  -438.39104356413975,\n","  -358.96224410340113,\n","  -239.92155249122806,\n","  -231.06905454300548,\n","  -105.86545871654836,\n","  -108.86971915098195,\n","  -607.4504185985,\n","  -457.3548193936489,\n","  -103.08479731974958,\n","  -101.10686054850848,\n","  -342.58639686849796,\n","  -111.49257983958493,\n","  -363.14619026589924,\n","  -242.25124561929007,\n","  -676.7481344719585,\n","  -1361.3831036980832,\n","  -1053.833201114238,\n","  17.18563148341251,\n","  18.738684351993754,\n","  -100.26850352680577,\n","  -1171.9814261708884,\n","  19.766637355476117,\n","  -98.00258313076411,\n","  -752.459218481303,\n","  19.629184638293122,\n","  -104.05138870452606,\n","  -525.8149913224006,\n","  -103.85655307485325,\n","  -103.6893163219191,\n","  -243.4853409702082,\n","  -502.63857319376103,\n","  -230.38336575801,\n","  -342.14758753173965,\n","  -107.44983964684725,\n","  -223.86705131005192,\n","  -109.32492618539715,\n","  -476.77395247251076,\n","  -100.43008755735714,\n","  19.173765663246705,\n","  -218.9723393349722,\n","  -347.63930336505075,\n","  -104.98146185326837,\n","  18.070232493517913,\n","  -105.55349109841825,\n","  -462.15627957435015,\n","  -102.11813075921748,\n","  19.745936845576583,\n","  -103.49084435684699,\n","  -358.9702767310685,\n","  -455.633049091531,\n","  -105.07880656005403,\n","  -101.81345327355733,\n","  -100.53348951155319,\n","  -98.52169729748289,\n","  -103.09158706035538,\n","  -376.97631778050635,\n","  -102.29234478679881,\n","  -207.89689165591514,\n","  18.797505348891924,\n","  -96.31782709022873,\n","  -98.88414710806201,\n","  -103.22019161727823,\n","  -232.17647767163382,\n","  -103.20319961376157,\n","  19.30944358762662,\n","  18.448344527980517,\n","  -228.89579881612275,\n","  -99.8124746506261,\n","  -560.7168477364334,\n","  -99.6848014906418,\n","  -96.2280101466657,\n","  -101.80300597123504,\n","  -1371.237072809303,\n","  -1072.998296934663,\n","  -103.81515696319896,\n","  -334.7060632962691,\n","  -101.97798735797979,\n","  -101.93672161787677,\n","  -214.70759371059597,\n","  -104.88255040945198,\n","  -437.0529064999721,\n","  -101.84084051441245,\n","  -239.67051110066012,\n","  -108.40760473701512,\n","  -222.0031249627499,\n","  -103.45360826080368,\n","  -219.86190807966477,\n","  -111.06275533282883,\n","  -94.15951855341396,\n","  -338.19636482200815,\n","  19.267516120112113,\n","  -96.83088553934213,\n","  -206.7103885881591,\n","  -103.23309253804001,\n","  17.44782322372596,\n","  -108.54183766494201,\n","  -230.393601933007,\n","  19.634326257109343,\n","  -516.5727285003808,\n","  -108.79983566489082,\n","  -665.6517027652228,\n","  -803.2204864289738,\n","  -97.05915654697307,\n","  -102.06982690963787,\n","  19.254962147413664,\n","  -103.77754807656541,\n","  -101.13439519003731,\n","  -235.1540188661144,\n","  -329.42893203402997,\n","  -222.2973387576708,\n","  -221.2194199454719,\n","  -209.11193032833847,\n","  -100.89734348415782,\n","  -100.96879894018356,\n","  -304.6761642200176,\n","  -602.371988472268,\n","  -228.17242149558558,\n","  -223.9480739770912,\n","  -95.9662500887238,\n","  -226.2632958643763,\n","  -563.6098234638539,\n","  -370.1071754345134,\n","  17.519856278166777,\n","  -254.26373032882256,\n","  -314.3006889949255,\n","  -94.70710715220541,\n","  -99.8177501672275,\n","  -99.26799749810193,\n","  -113.55350582084331,\n","  -1238.0776839535436,\n","  -1238.9509441304485,\n","  -1332.003416500234,\n","  -1181.657769531565,\n","  -103.11507381924972,\n","  -891.3054152389622,\n","  -106.5291110617847,\n","  -97.59060828341234,\n","  -105.61078550400292,\n","  -215.5430766273722,\n","  -104.68737646885363,\n","  -94.7161293628266,\n","  18.420757182159672,\n","  17.756284811361844],\n"," 'avg_reward_100': [-1364.7476202295459,\n","  -1444.7855446116282,\n","  -1519.2912385912898,\n","  -1387.9890833088596,\n","  -1301.4630758036105,\n","  -1294.34239411411,\n","  -1238.2849368197888,\n","  -1239.4561844825785,\n","  -1244.9709177572627,\n","  -1216.136051737011,\n","  -1211.2852393725714,\n","  -1192.7257477885744,\n","  -1177.2283270210123,\n","  -1192.5097576984933,\n","  -1189.7868869493286,\n","  -1193.6389451349555,\n","  -1199.617249221867,\n","  -1204.766223801139,\n","  -1203.7732082934895,\n","  -1203.0245851618943,\n","  -1181.6444826960146,\n","  -1176.9438279067856,\n","  -1164.093075670752,\n","  -1151.7221399440207,\n","  -1139.8529164668553,\n","  -1132.7754491270925,\n","  -1121.6884845423917,\n","  -1111.6854129359556,\n","  -1099.0135402564206,\n","  -1090.8365800580898,\n","  -1075.7605716538644,\n","  -1064.6832582424695,\n","  -1054.565916038005,\n","  -1045.0595850587579,\n","  -1036.085586173406,\n","  -1027.41273812099,\n","  -1019.3174017636289,\n","  -1008.5738719155564,\n","  -999.7353369606288,\n","  -990.0389208266508,\n","  -980.7167799991386,\n","  -971.9387283885399,\n","  -965.1031114632593,\n","  -956.9085570994531,\n","  -951.5584568775334,\n","  -943.9478989530218,\n","  -939.2872876558362,\n","  -931.6316154957412,\n","  -925.1140846059715,\n","  -918.9511837155349,\n","  -910.6779099952338,\n","  -902.5894635240061,\n","  -894.8416691789886,\n","  -884.9382545900589,\n","  -877.4262071863222,\n","  -870.1383984864884,\n","  -863.2102727104462,\n","  -855.8858032424065,\n","  -847.4633700366606,\n","  -837.3376730775701,\n","  -827.3988432655281,\n","  -815.7612080308671,\n","  -804.540708207377,\n","  -801.4611724322381,\n","  -796.1672285393367,\n","  -785.6659795814643,\n","  -775.4486792973902,\n","  -769.0830574969654,\n","  -759.5527607193222,\n","  -753.8898097128447,\n","  -746.6836327537806,\n","  -745.7123063887551,\n","  -754.1461529272391,\n","  -758.1959779027391,\n","  -747.857556444257,\n","  -737.7707638022011,\n","  -729.4915136687545,\n","  -735.1644612649357,\n","  -725.608371408981,\n","  -717.7632990555034,\n","  -718.1916437397725,\n","  -709.1938287595522,\n","  -701.9029559878048,\n","  -699.806670694169,\n","  -692.7954928398241,\n","  -685.9454210198485,\n","  -680.8596729733009,\n","  -678.8344332030789,\n","  -673.7956571643703,\n","  -670.1106786128967,\n","  -663.9275924704126,\n","  -659.1443257186695,\n","  -653.2322891645483,\n","  -651.3550728167608,\n","  -645.555862445609,\n","  -638.6315954861418,\n","  -634.3052114021091,\n","  -631.3800490752004,\n","  -626.0628916285142,\n","  -619.6215603872938,\n","  -607.0296190959826,\n","  -596.4029472017891,\n","  -580.741102243875,\n","  -570.6028167008037,\n","  -562.084134686546,\n","  -553.0864475971906,\n","  -548.6233761575672,\n","  -537.1976150419467,\n","  -525.324861735135,\n","  -516.763974054703,\n","  -506.12141987039604,\n","  -497.26662233735345,\n","  -491.1237927370559,\n","  -478.23503261986656,\n","  -468.79733457181544,\n","  -456.09516133913297,\n","  -444.10563846391074,\n","  -432.1714920185037,\n","  -421.34470464311846,\n","  -411.77846196321894,\n","  -405.2700696255724,\n","  -394.2946744163663,\n","  -385.2964257063064,\n","  -378.9134775121755,\n","  -371.3616867285329,\n","  -367.41046754956704,\n","  -360.0730415110718,\n","  -352.61929681691674,\n","  -346.1953158243346,\n","  -351.37063920936254,\n","  -355.8658189834382,\n","  -349.6911051281779,\n","  -345.7300561061891,\n","  -339.4363293523331,\n","  -333.14600032779737,\n","  -328.05444570203906,\n","  -321.8244182771472,\n","  -320.0843146667782,\n","  -314.46401298518856,\n","  -310.7419311801801,\n","  -305.74769575856374,\n","  -301.8473408846513,\n","  -296.1018049612446,\n","  -292.25499684748337,\n","  -286.2040839296809,\n","  -281.1309511917151,\n","  -277.2639231600822,\n","  -271.35309775916835,\n","  -266.1986805955915,\n","  -262.0960940806317,\n","  -258.15828276621033,\n","  -253.08301759905913,\n","  -249.24887234332778,\n","  -247.95223554889003,\n","  -243.03813581247348,\n","  -243.51077389752106,\n","  -239.84641996164908,\n","  -242.11902655365992,\n","  -246.56160897691564,\n","  -245.13298501747306,\n","  -243.84299274113945,\n","  -242.59178853249986,\n","  -242.54086682175566,\n","  -237.477706587671,\n","  -235.25569858239567,\n","  -237.51913992953848,\n","  -238.73104471163006,\n","  -237.5173749423998,\n","  -238.49356844728737,\n","  -235.87107997946995,\n","  -234.45825551267887,\n","  -230.73753581015947,\n","  -223.14742465790133,\n","  -214.89081686171482,\n","  -217.30215391631984,\n","  -218.44920326072705,\n","  -219.70915118410275,\n","  -213.6254351570324,\n","  -217.5241732849323,\n","  -216.36894889084297,\n","  -211.3869940093182,\n","  -214.7262927456503,\n","  -214.63284993012712,\n","  -210.3728775185754,\n","  -210.32699196280788,\n","  -210.42563385779715,\n","  -220.37155728763048,\n","  -227.73468099699733,\n","  -238.75088150441957,\n","  -247.14598332441784,\n","  -247.10263566614182,\n","  -253.77701930543094,\n","  -253.74906115419483,\n","  -249.95722771230382,\n","  -250.00903469177032,\n","  -252.35620311467648,\n","  -251.2133534860153,\n","  -248.68412174599308,\n","  -247.45009955563881,\n","  -247.45323903246035],\n"," 'steps': [200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200,\n","  200]}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model = DDQN(env=env, lr=lr, gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay)\n","model.train(training_episodes,save_episodes=True, save_interval=20)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNyxQA+uo+CZWGrCNzPMfCN","provenance":[]},"kernelspec":{"display_name":"Python 3.8.18 ('gpu_env2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"vscode":{"interpreter":{"hash":"f1e7a337613e586b6b6f0763924cba37d1373b7b0c0ecab0f2ace611c5143063"}}},"nbformat":4,"nbformat_minor":0}
